# -*- coding: utf-8 -*-
"""Topic Naming - Movie Quotes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1anjfSi5IYLNm2Ibipz1sp9GeTPPatKup

# Topic Naming - Movie Quotes

This notebook automatically generates topic names that a collection of text is talking about.

How to use it.

1. Add a "Secrets" named `OPENAI_API_KEY` with your [OpenAI API key](https://platform.openai.com/account/api-keys) (or use `LLMFOUNDRY_TOKEN`)
2. Run all the cells in this notebook. Then download the generated files:
    1. `docexplore.xlsx` has a `topics` sheet with the topics and subtopics
    2. `docexplore.json` has the matches data compatible with [gramex-docexplore](https://gramener.com/gramex-docexplore/)
"""

!wget --no-clobber https://raw.githubusercontent.com/sanand0/actor-lookalikes/main/movie-quotes-catchphrases.csv

import os
from google.colab import userdata

# Upload an Excel file that contains a text column
filename = 'movie-quotes-catchphrases.csv'

# Name of the text column
text_column = 'Catchphrase'

# Pick the number of subtopics you want
n_subtopics = 25

# OPTIONAL: Pick the rough number of topics you want
n_topics = int(n_subtopics ** 0.5)

# OPTIONAL: Ignore any similarities less than min_similarity.
# 0.75 is a good setting for text-embedding-ada-002
# 0.40 is a good setting for text-embedding-3-small/large
min_similarity = 0.40

# OPTIONAL: Change the OpenAI API base, e.g.
openai_api_base = "https://llmfoundry.straive.com/openai/v1/"
# openai_api_base = "https://api.openai.com/v1/"

# OPTIONAL: Change the OpenAI API key, e.g.
openai_api_key = f'{userdata.get("LLMFOUNDRY_TOKEN")}:topic-naming'
# openai_api_key = userdata.get('OPENAI_API_KEY')

# OPTIONAL: Change the chat model, e.g.
# chat_model_name = 'gpt-4-turbo'
chat_model_name = 'gpt-4o-mini'

# Tested with langchain==0.14 openai==1.10.0 tiktoken==0.5.2 scikit-learn==1.2.2
!pip install langchain langchain-openai openai tiktoken scikit-learn

# Set up langchain embeddings
import os
from langchain.embeddings.cache import CacheBackedEmbeddings
from langchain.storage.file_system import LocalFileStore
from langchain_openai import OpenAIEmbeddings

file_store = LocalFileStore('langchain-embeddings')
base = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_base=openai_api_base,
    openai_api_key=openai_api_key,
)
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base, file_store, namespace=base.model)

# Define classify() and cluster() functions
import numpy as np
from typing import List
from sklearn.cluster import BisectingKMeans
from sklearn.metrics import silhouette_score, silhouette_samples


def classify(docs: List[str], topics: List[str], **kwargs):
    doc_embed = np.array(cached_embeddings.embed_documents(docs))
    topic_embed = np.array(cached_embeddings.embed_documents(topics))
    return np.dot(doc_embed, topic_embed.T)


def cluster(docs: List[str], n: int = 20, **kwargs):
    # Cluster the documents. BisectingKMeans looks at the data a bit before partitioning
    cluster_model = BisectingKMeans(init='k-means++', n_clusters=n, n_init=10, max_iter=1000)
    doc_embed = np.array(cached_embeddings.embed_documents(docs))
    cluster_model.fit(doc_embed)
    # Calculate the distance from each point to each centroid
    distances = np.linalg.norm(doc_embed[:, np.newaxis] - cluster_model.cluster_centers_, axis=2)
    return {
        "label": cluster_model.labels_,
        "score": silhouette_score(doc_embed, cluster_model.labels_),
        "scores": silhouette_samples(doc_embed, cluster_model.labels_),
        "centroid": np.argmin(distances, axis=0),
    }

# Create topics by clustering
import pandas as pd

docs = pd.read_csv(filename).dropna(subset=text_column).fillna('').astype(str)
result = cluster(docs[text_column].tolist(), n=n_subtopics)
docs['cluster'] = result['label']
docs['score'] = result['scores']
clusters = (
    docs.groupby('cluster')
    .apply(lambda group: group.nlargest(3, 'score')[text_column].tolist())
    .tolist()
)

# This is what the cluster looks like
docs

# Automatically generate subtopics from clusters

import json
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

chat_model = ChatOpenAI(
    model=chat_model_name,
    temperature=0,
    openai_api_base=openai_api_base,
    openai_api_key=openai_api_key,
)
messages = [
    HumanMessage(
        content=f'''Here are {len(clusters)} clusters of documents.
Suggest 2-4 word topic names for each cluster.
Return a JSON string array of length {len(clusters)}.

{json.dumps(clusters, indent=2)}'''
    )
]

# Get the ChatGPT response
subtopic_response = chat_model.invoke(messages)

# Extract everything inside ```json ... ```
import re

match = re.search(r'```json(.*?)```', subtopic_response.content, re.DOTALL)
subtopics = json.loads(match.group(1) if match else subtopic_response.content)
subtopics

# Create higher-level topic groups
messages = [
    HumanMessage(
        content=f'''Cluster these topics into {n_topics} groups.
Return a JSON object with keys as a 2-4 word group name and values as arrays of topics.
Ensure at least 2 topics per group.

{json.dumps(subtopics, indent=2)}'''
    )
]

# Get the ChatGPT response
topic_response = chat_model.invoke(messages)

match = re.search(r'```json(.*?)```', topic_response.content, re.DOTALL)
topics = json.loads(match.group(1) if match else topic_response.content)
topics = pd.DataFrame([
    {'topic': topic, 'subtopic': subtopic}
    for topic, subtopics in topics.items()
    for subtopic in subtopics
])
topics

# Let's example a subtopic
index = 0
print(topics.subtopic.iloc[index])
docs[docs.cluster == index].sort_values("score", ascending=False)

# Calculate similarity between each doc and topic

data = {
    'docs': docs.to_dict(orient='records'),
    'topics': topics.to_dict(orient='records'),
}

# Loop through each row and column in maches and create a {doc, topic, similarity} list
matches = data['matches'] = []
similarity = classify(
    [row[text_column] for row in data['docs']],
    [row['subtopic'] for row in data['topics']]
)

for row in range(len(similarity)):
    for col in range(len(similarity[row])):
        if similarity[row][col] > min_similarity:
            matches.append({'doc': row, 'topic': col, 'similarity': similarity[row][col]})

# Save as Excel
with pd.ExcelWriter('docexplore.xlsx') as writer:
    docs.to_excel(writer, sheet_name='docs', index=False)
    topics.to_excel(writer, sheet_name='topics', index=False)
    grid = pd.DataFrame(matches).pivot_table(index='doc', columns='topic', values='similarity')
    grid.index = pd.Series(grid.index).replace(dict(enumerate(docs[text_column].tolist())))
    grid.columns = pd.Series(grid.columns).replace(dict(enumerate(topics['subtopic'].tolist())))
    grid.index.name = text_column
    grid.reset_index().to_excel(writer, sheet_name='matches', index=False)

# Save as JSON

with open("docexplore.json", "w") as handle:
    handle.write(json.dumps(data, indent=2))

"""---

Now the data is saved in:

- docexplore.xlsx (with sheets: docs, topics, matches)
- docexplore.json (with keys: docs, topics, matches)
"""